{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling from the FNN Output\n",
    "\n",
    "This is only for sampling from the FNN output, needs to be integrated in the general fnn sampling notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cu124\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import os\n",
    "from utils import createFlowDataset, subps \n",
    "from glob import glob\n",
    "from time import time\n",
    "\n",
    "import sys\n",
    "\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "\n",
    "print(torch.__version__)  # E.g., '1.10.0'\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "from fnn import microns\n",
    "from numpy import full, concatenate\n",
    "\n",
    "model, ids = microns.scan(session=6, scan_idx=4)\n",
    "SUFFIX = \"fnn07_6_4\" \n",
    "\n",
    "################# SET PARAMS ##########################\n",
    "n_fmaps_to_sample = 1\n",
    "samples_per_fmap = 2000\n",
    "seed = 3\n",
    "\n",
    "################# MORE PARAMS #########################\n",
    "LAYER_TYPE = 'act'\n",
    "MAX_SIDE = 32\n",
    "\n",
    "# Flow stimuli parameters\n",
    "scl_factor = 0.7\n",
    "N_INSTANCES = 3\n",
    "trial_len = 75 // 2  # Number of frames\n",
    "stride = 1\n",
    "\n",
    "## SAMPLING\n",
    "fmap_samp_method = 'rnd'\n",
    "neur_samp_method = 'rnd'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the input shape expected by the model\n",
    "input_shape = (144, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tot_stims 88\n",
      "frames_per_stim 37\n",
      "*INSTANCE 0 ...........\n",
      "*INSTANCE 1 ...........\n",
      "*INSTANCE 2 ...........\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x100 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "############# LOAD FLOW STIM FRAMES #################\n",
    "\n",
    "orig_shape = (800, 600)\n",
    "\n",
    "mydirs = list(map(str, range(0, 360, 45)))\n",
    "categories = ['grat_W12', 'grat_W1', 'grat_W2',\n",
    "              'neg1dotflow_D1_bg', 'neg3dotflow_D1_bg', 'neg1dotflow_D2_bg', 'neg3dotflow_D2_bg',\n",
    "              'pos1dotflow_D1_bg', 'pos3dotflow_D1_bg', 'pos1dotflow_D2_bg', 'pos3dotflow_D2_bg']\n",
    "\n",
    "topdir = 'flowstims'\n",
    "NDIRS = len(mydirs)\n",
    "tot_stims = len(categories) * NDIRS\n",
    "print('tot_stims', tot_stims, flush=True)\n",
    "frames_per_stim = (trial_len // stride)\n",
    "print('frames_per_stim', frames_per_stim)\n",
    "\n",
    "# Create flow datasets (placeholder function)\n",
    "flow_datasets = createFlowDataset(categories, topdir, mydirs, orig_shape, input_shape,\n",
    "                                  scl_factor, N_INSTANCES, trial_len, stride)\n",
    "\n",
    "# Show example of sequence of frames generated for a stimulus trial\n",
    "n_frames_to_show = 4\n",
    "interval = 8\n",
    "\n",
    "f, axes = subps(1, n_frames_to_show, 1, 1)\n",
    "for i in range(n_frames_to_show):\n",
    "    ax = axes[i]\n",
    "    img = flow_datasets[0][i * interval].reshape(input_shape)\n",
    "    ax.imshow(img, vmin=0, vmax=255, cmap='gray')\n",
    "    ax.axis('off')\n",
    "\n",
    "f.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3256, 36864)\n",
      "36864\n",
      "11.0\n"
     ]
    }
   ],
   "source": [
    "print(flow_datasets[0].shape)\n",
    "print(144*256)\n",
    "print(3256 / 8 / 37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_flow_img(raveled_1chan_img):\n",
    "    img = raveled_1chan_img.reshape((37, input_shape[0], input_shape[1]))\n",
    "    #img = np.stack([img, img, img], axis=0)  # Convert to 3 channels\n",
    "    img = img.astype(np.uint8)\n",
    "    return img\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tot # of images: 88 * 37 = 3256\n",
      "(3256, 36864)\n",
      "INSTANCE 0\n",
      "(0.5s) (0.4s) (0.4s) (0.4s) (0.4s) (0.5s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.5s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.5s) (0.4s) (0.5s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.5s) (0.7s) (0.5s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.5s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s)  Tot time = 37.4\n",
      "(3256, 8221, 1, 1)\n",
      "(3256, 36864)\n",
      "INSTANCE 1\n",
      "(0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.5s) (0.5s) (0.5s) (0.5s) (0.5s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.5s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.5s) (0.5s) (0.5s) (0.5s) (0.5s) (0.5s) (0.5s) (0.5s) (0.5s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s)  Tot time = 38.1\n",
      "(3256, 8221, 1, 1)\n",
      "(3256, 36864)\n",
      "INSTANCE 2\n",
      "(0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.5s) (0.5s) (0.5s) (0.5s) (0.5s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s) (0.4s)  Tot time = 37.2\n",
      "(3256, 8221, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "####################### COMPUTE ################\n",
    "\n",
    "\n",
    "\n",
    "TOL = 0\n",
    "\n",
    "n_orig_imgs = tot_stims\n",
    "n_shifts = frames_per_stim\n",
    "n_shifted_imgs = n_orig_imgs * n_shifts\n",
    "\n",
    "print('tot # of images:', n_orig_imgs, '*', n_shifts, '=', n_shifted_imgs)\n",
    "\n",
    "\n",
    "layer_outputs = []\n",
    "instance_layer_outputs = []\n",
    "\n",
    "for li in range(1):\n",
    "    shape = [n_shifted_imgs] + [8221,1,1]\n",
    "    layer_outputs.append(np.zeros(shape, dtype='float32'))\n",
    "    shape_inst = np.append([N_INSTANCES], shape)\n",
    "    instance_layer_outputs.append(np.zeros(shape_inst, dtype='float32'))\n",
    "\n",
    "for insti in range(N_INSTANCES):\n",
    "    extX = flow_datasets[insti]\n",
    "    print(extX.shape)\n",
    "    assert extX.shape[0] == n_shifted_imgs\n",
    "\n",
    "    print('INSTANCE', insti)\n",
    "    start0 = time()\n",
    "    layer_output = []\n",
    "    layer_output.append([])\n",
    "\n",
    "    for seq_idx in range(int(len(extX)/37)):\n",
    "        start = time()\n",
    "\n",
    "        # Prepare batch\n",
    "        sequence = extX[seq_idx*37:(seq_idx+1)*37]\n",
    "\n",
    "        sequence = reshape_flow_img(extX[seq_idx*37:(seq_idx+1)*37])\n",
    "\n",
    "\n",
    "        # Collect outputs per layer\n",
    "\n",
    "        output = model.predict(stimuli=sequence)\n",
    "        #print(output.shape)\n",
    "        #output = output.reshape((37, 144, 256))\n",
    "        \n",
    "        # Crop the output if needed\n",
    "\n",
    "\n",
    "        layer_output[0].append(output)\n",
    "\n",
    "        print('(%.1fs) ' % (time() - start), end='', flush=True)\n",
    "    print(' Tot time = %.1f' % (time() - start0), flush=True)\n",
    "\n",
    "    # After processing all batches for this instance, concatenate outputs\n",
    "    for li in range(1):\n",
    "        # print(li)\n",
    "        # print([l.shape for l in layer_output[li]])\n",
    "        layer_output[li] = np.expand_dims(np.expand_dims(np.concatenate(layer_output[li], axis=0), axis=2), axis=3)\n",
    "        print(layer_output[li].shape)\n",
    "        layer_outputs[li] += layer_output[li]\n",
    "        instance_layer_outputs[li][insti] = layer_output[li]\n",
    "\n",
    "\n",
    "# Average over instances\n",
    "for li in range(1):\n",
    "    layer_outputs[li] /= N_INSTANCES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activities per img: 0"
     ]
    }
   ],
   "source": [
    "################### SUMMARIZE ACTIVITY ###########\n",
    "\n",
    "print('Activities per img:', end=' ')\n",
    "all_neurons_maxs = []\n",
    "all_neurons_means = []\n",
    "all_per_img_output = []\n",
    "for li in range(1):\n",
    "    print(li, end='', flush=True)\n",
    "    layer_output_ = layer_outputs[li].copy()\n",
    "\n",
    "    layer_output_[layer_output_ < 0] = 0\n",
    "\n",
    "    nfmaps = layer_output_.shape[3]\n",
    "    # Reshape to [n_orig_imgs, n_shifts, nfmaps, -1]\n",
    "    orig_per_img_output = np.moveaxis(layer_output_, -1, 1).reshape([n_orig_imgs, n_shifts, nfmaps, -1])\n",
    "    orig_per_img_output = np.moveaxis(orig_per_img_output, 1, -1)\n",
    "\n",
    "    # Normalize each image by the max\n",
    "    layer_output_ /= np.maximum(layer_output_.max((1, 2, 3), keepdims=True), 1e-8)\n",
    "\n",
    "    per_img_output = np.moveaxis(layer_output_, -1, 1).reshape([n_orig_imgs, n_shifts, nfmaps, -1])\n",
    "    per_img_output = np.moveaxis(per_img_output, 1, -1)\n",
    "\n",
    "    tot_n_neurons = np.prod(layer_output_.shape[1:])\n",
    "\n",
    "    neurons_maxs = np.zeros(per_img_output.shape[1:3])\n",
    "    neurons_means = np.zeros(per_img_output.shape[1:3])\n",
    "\n",
    "    for imi in range(n_orig_imgs):\n",
    "        im_avgs = per_img_output[imi].mean(2)  # Averaging across time\n",
    "        neurons_maxs = np.maximum(neurons_maxs, im_avgs)\n",
    "        neurons_means += im_avgs\n",
    "    neurons_means /= n_orig_imgs\n",
    "\n",
    "    idxs = neurons_maxs.mean(1).argsort()\n",
    "\n",
    "    if li == 0:\n",
    "        all_neurons_maxs = neurons_maxs\n",
    "        all_neurons_means = neurons_means\n",
    "        all_per_img_output = orig_per_img_output\n",
    "    else:\n",
    "        all_neurons_maxs = np.concatenate([all_neurons_maxs, neurons_maxs], 0)\n",
    "        all_neurons_means = np.concatenate([all_neurons_means, neurons_means], 0)\n",
    "        all_per_img_output = np.concatenate([all_per_img_output, orig_per_img_output], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2000\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "############# SAMPLE NEURONS ###########\n",
    "\n",
    "nfmaps, n_neurons_per_fmap = all_neurons_maxs.shape\n",
    "layer_is_per_fmap = np.concatenate([li * np.ones(nf) for li, nf in enumerate([1])])\n",
    "np.random.seed(seed)\n",
    "\n",
    "maxsmean = all_neurons_maxs.mean(1)\n",
    "nonzero_indices = (~np.isclose(maxsmean, 0)).sum()\n",
    "n_fmaps_to_sample_ = min(n_fmaps_to_sample, nonzero_indices)\n",
    "print(n_fmaps_to_sample)\n",
    "if fmap_samp_method == 'maxFr' or fmap_samp_method == 'rnd':\n",
    "    probabilities = maxsmean / maxsmean.sum()\n",
    "    top_fmaps = np.random.choice(range(nfmaps), n_fmaps_to_sample_, replace=False)#TODO, p=probabilities)\n",
    "else:\n",
    "    raise ValueError('Invalid fmap_samp_method')\n",
    "\n",
    "# Pick active neurons in each of these feature maps\n",
    "sampled_neurons = []\n",
    "\n",
    "samples_per_fmap = min(samples_per_fmap, all_neurons_means.shape[1])\n",
    "print(samples_per_fmap)\n",
    "for fi in top_fmaps:\n",
    "    if neur_samp_method == 'maxNr' or neur_samp_method == 'rnd':\n",
    "        neuron_vals = all_neurons_maxs[fi]\n",
    "        nonzero_neurons = (~np.isclose(neuron_vals, 0)).sum()\n",
    "        samples_per_fmap_ = min(samples_per_fmap, nonzero_neurons)\n",
    "        probabilities = neuron_vals / neuron_vals.sum()\n",
    "        top_nis = np.random.choice(range(n_neurons_per_fmap), samples_per_fmap_, replace=False)#TODO, p=probabilities)\n",
    "    else:\n",
    "        raise ValueError('Invalid neur_samp_method')\n",
    "    sampled_neurons += list(fi * n_neurons_per_fmap + top_nis)\n",
    "sampled_neurons = np.array(sampled_neurons)\n",
    "n_neurons_to_pick = len(sampled_neurons)\n",
    "print(n_neurons_to_pick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 11, 8, 37)\n",
      "(2000, 5)\n",
      "tensor4d_fnn07_6_4_seed3_rnd_rnd.npy Saved.\n",
      "neurons_used_fnn07_6_4_seed3_rnd_rnd.npy Saved.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "######### BUILD TENSOR ##########\n",
    "\n",
    "def get_neuron_pos(ni):\n",
    "    \"\"\"From sampled indices ni, get original indices back (layer index, fmap, posi, posj, raveled_idx)\"\"\"\n",
    "    fi = ni // n_neurons_per_fmap\n",
    "    li = int(layer_is_per_fmap[fi])\n",
    "    ij = ni % n_neurons_per_fmap\n",
    "    h, w, _ = (8221, 1, 1)\n",
    "    ii = ij // w\n",
    "    jj = ij % w\n",
    "    return li, fi, ii, jj, ij\n",
    "\n",
    "assert n_orig_imgs // NDIRS == len(categories)\n",
    "\n",
    "tensorX = np.zeros((n_neurons_to_pick, len(categories), NDIRS, n_shifts))\n",
    "neurons_used = np.empty((n_neurons_to_pick, 5), dtype='int')\n",
    "\n",
    "# Collect PSTs for those sampled neurons\n",
    "for nii, ni in enumerate(sampled_neurons):\n",
    "    li, fi, ii, jj, posi = get_neuron_pos(ni)\n",
    "    neurons_used[nii] = [li, fi, ii, jj, posi]\n",
    "\n",
    "    for cati in range(len(categories)):\n",
    "        pst = all_per_img_output[cati * NDIRS: (cati + 1) * NDIRS, fi, posi, :]\n",
    "        tensorX[nii, cati] = pst\n",
    "\n",
    "#SUFFIX = f\"{model_name}_{LAYER_TYPE}_i{N_INSTANCES}_n{n_neurons_to_pick}_SCL{str(scl_factor).replace('.', '_')}_TL{trial_len}_{'_'.join(block)}_{fmap_samp_method}_{neur_samp_method}\"\n",
    "\n",
    "\n",
    "#print(SUFFIX)\n",
    "\n",
    "\n",
    "if seed > 0:\n",
    "    SUFFIX += f'_seed{seed}'\n",
    "print(tensorX.shape)\n",
    "print(neurons_used.shape)\n",
    "\n",
    "SUFFIX += f'_{fmap_samp_method}_{neur_samp_method}'\n",
    "\n",
    "directory = '../data/sampled_data'\n",
    "os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "if os.path.exists(f'../data/sampled_data/tensor4d_{SUFFIX}.npy'):\n",
    "    print(\"Files already exist, please delete them to prevent conflicts.\")\n",
    "else:\n",
    "    \n",
    "    np.save(f'../data/sampled_data/tensor4d_{SUFFIX}.npy', tensorX)\n",
    "    print(f'tensor4d_{SUFFIX}.npy Saved.')\n",
    "\n",
    "    np.save(f'../data/sampled_data/neurons_used_{SUFFIX}.npy', neurons_used)\n",
    "    print(f'neurons_used_{SUFFIX}.npy Saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
