{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "core shape:  torch.Size([1, 320, 33, 65])\n",
      "input shape of readout:  (320, 33, 65)\n",
      "model name:  l1a5_022723_2layer_16_320_clamp_norm_depthsep_pool.pt\n",
      "loaded model minimodels/notebooks/checkpoints/l1a5_022723_2layer_16_320_clamp_norm_depthsep_pool.pt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from minimodels.minimodel import data, model_trainer, model_builder\n",
    "\n",
    "device = torch.device('cuda')\n",
    "mouse_id = 0\n",
    "\n",
    "data_path = 'minimodels/notebooks/data'\n",
    "weight_path = 'minimodels/notebooks/checkpoints'\n",
    "np.random.seed(1)\n",
    "\n",
    "# build model\n",
    "nlayers = 2\n",
    "nconv1 = 16\n",
    "nconv2 = 320\n",
    "model, in_channels = model_builder.build_model(NN=6636, n_layers=nlayers, n_conv=nconv1, n_conv_mid=nconv2)\n",
    "model_name = model_builder.create_model_name(\"L1_A5\", data.exp_date[mouse_id], n_layers=nlayers, in_channels=in_channels)\n",
    "\n",
    "model_path = os.path.join(weight_path, model_name)\n",
    "model = model.to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "print('loaded model', model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import os\n",
    "from utils import createFlowDataset, subps  \n",
    "from glob import glob\n",
    "from time import time\n",
    "\n",
    "import sys\n",
    "\n",
    "# Import the ResNet50 model\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "################# SET PARAMS ##########################\n",
    "block = ['readout'] # select from readout\n",
    "model_name = 'mini' # \n",
    "n_fmaps_to_sample = 40\n",
    "samples_per_fmap = 2000\n",
    "seed = 1\n",
    "\n",
    "################ MORE PARAMS ##########################\n",
    "# I suggest leaving the following unchanged for comparability\n",
    "LAYER_TYPE = 'act'\n",
    "MAX_SIDE = 32\n",
    "\n",
    "# Flow stimuli parameters\n",
    "scl_factor = 0.5\n",
    "N_INSTANCES = 3\n",
    "trial_len = 75 // 2  # Number of frames\n",
    "stride = 1\n",
    "\n",
    "## SAMPLING\n",
    "fmap_samp_method = 'maxFr'\n",
    "neur_samp_method = 'maxNr'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 2924596\n"
     ]
    }
   ],
   "source": [
    "############## LOAD MODEL ############################\n",
    "\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "# Define the input shape expected by the model\n",
    "input_shape = (66, 130)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'Total number of parameters: {total_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tot_stims 88\n",
      "frames_per_stim 37\n",
      "*INSTANCE 0 ."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..........\n",
      "*INSTANCE 1 ...........\n",
      "*INSTANCE 2 ...........\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x100 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "############# LOAD FLOW STIM FRAMES #################\n",
    "\n",
    "orig_shape = (800, 600)\n",
    "scl_factor = 0.4\n",
    "\n",
    "mydirs = list(map(str, range(0, 360, 45)))\n",
    "categories = ['grat_W12', 'grat_W1', 'grat_W2',\n",
    "              'neg1dotflow_D1_bg', 'neg3dotflow_D1_bg', 'neg1dotflow_D2_bg', 'neg3dotflow_D2_bg',\n",
    "              'pos1dotflow_D1_bg', 'pos3dotflow_D1_bg', 'pos1dotflow_D2_bg', 'pos3dotflow_D2_bg']\n",
    "\n",
    "topdir = 'flowstims'\n",
    "NDIRS = len(mydirs)\n",
    "tot_stims = len(categories) * NDIRS\n",
    "print('tot_stims', tot_stims, flush=True)\n",
    "frames_per_stim = (trial_len // stride)\n",
    "print('frames_per_stim', frames_per_stim)\n",
    "\n",
    "# Create flow datasets (placeholder function)\n",
    "flow_datasets = createFlowDataset(categories, topdir, mydirs, orig_shape, input_shape,\n",
    "                                  scl_factor, N_INSTANCES, trial_len, stride)\n",
    "\n",
    "# Show example of sequence of frames generated for a stimulus trial\n",
    "n_frames_to_show = 4\n",
    "interval = 8\n",
    "\n",
    "f, axes = subps(1, n_frames_to_show, 1, 1)\n",
    "for i in range(n_frames_to_show):\n",
    "    ax = axes[i]\n",
    "    img = flow_datasets[0][i * interval].copy().reshape(input_shape)\n",
    "    ax.imshow(img, vmin=0, vmax=255, cmap='gray')\n",
    "    ax.axis('off')\n",
    "\n",
    "f.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tot # of images: 88 * 37 = 3256\n",
      "batchsize 8 -- # of batches: 407\n",
      "INSTANCE 0\n",
      "0 Output shape: torch.Size([8, 6636])\n",
      "(0.3s) 1 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 2 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 3 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 4 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 5 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 6 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 7 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 8 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 9 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 10 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 11 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 12 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 13 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 14 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 15 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 16 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 17 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 18 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 19 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 20 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 21 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 22 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 23 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 24 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 25 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 26 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 27 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 28 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 29 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 30 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 31 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 32 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 33 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 34 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 35 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 36 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 37 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 38 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 39 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 40 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 41 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 42 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 43 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 44 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 45 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 46 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 47 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 48 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 49 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 50 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 51 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 52 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 53 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 54 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 55 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 56 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 57 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 58 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 59 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 60 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 61 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 62 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 63 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 64 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 65 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 66 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 67 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 68 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 69 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 70 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 71 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 72 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 73 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 74 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 75 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 76 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 77 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 78 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 79 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 80 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 81 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 82 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 83 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 84 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 85 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 86 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 87 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 88 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 89 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 90 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 91 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 92 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 93 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 94 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 95 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 96 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 97 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 98 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 99 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 100 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 101 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 102 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 103 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 104 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 105 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 106 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 107 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 108 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 109 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 110 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 111 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 112 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 113 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 114 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 115 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 116 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 117 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 118 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 119 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 120 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 121 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 122 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 123 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 124 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 125 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 126 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 127 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 128 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 129 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 130 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 131 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 132 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 133 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 134 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 135 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 136 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 137 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 138 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 139 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 140 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 141 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 142 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 143 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 144 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 145 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 146 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 147 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 148 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 149 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 150 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 151 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 152 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 153 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 154 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 155 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 156 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 157 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 158 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 159 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 160 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 161 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 162 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 163 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 164 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 165 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 166 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 167 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 168 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 169 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 170 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 171 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 172 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 173 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 174 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 175 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 176 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 177 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 178 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 179 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 180 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 181 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 182 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 183 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 184 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 185 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 186 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 187 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 188 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 189 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 190 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 191 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 192 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 193 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 194 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 195 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 196 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 197 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 198 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 199 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 200 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 201 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 202 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 203 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 204 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 205 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 206 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 207 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 208 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 209 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 210 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 211 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 212 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 213 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 214 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 215 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 216 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 217 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 218 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 219 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 220 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 221 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 222 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 223 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 224 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 225 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 226 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 227 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 228 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 229 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 230 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 231 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 232 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 233 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 234 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 235 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 236 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 237 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 238 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 239 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 240 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 241 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 242 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 243 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 244 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 245 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 246 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 247 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 248 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 249 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 250 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 251 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 252 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 253 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 254 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 255 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 256 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 257 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 258 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 259 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 260 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 261 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 262 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 263 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 264 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 265 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 266 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 267 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 268 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 269 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 270 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 271 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 272 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 273 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 274 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 275 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 276 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 277 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 278 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 279 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 280 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 281 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 282 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 283 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 284 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 285 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 286 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 287 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 288 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 289 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 290 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 291 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 292 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 293 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 294 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 295 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 296 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 297 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 298 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 299 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 300 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 301 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 302 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 303 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 304 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 305 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 306 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 307 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 308 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 309 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 310 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 311 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 312 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 313 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 314 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 315 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 316 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 317 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 318 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 319 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 320 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 321 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 322 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 323 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 324 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 325 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 326 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 327 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 328 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 329 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 330 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 331 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 332 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 333 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 334 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 335 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 336 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 337 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 338 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 339 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 340 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 341 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 342 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 343 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 344 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 345 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 346 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 347 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 348 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 349 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 350 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 351 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 352 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 353 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 354 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 355 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 356 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 357 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 358 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 359 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 360 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 361 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 362 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 363 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 364 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 365 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 366 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 367 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 368 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 369 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 370 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 371 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 372 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 373 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 374 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 375 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 376 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 377 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 378 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 379 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 380 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 381 Output shape: torch.Size([8, 6636])\n",
      "(0.0s) 382 Output shape: torch.Size([8, 6636])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 81\u001b[39m\n\u001b[32m     69\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m li \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m):\n\u001b[32m     70\u001b[39m \u001b[38;5;250m        \u001b[39m\u001b[33;03m\"\"\"# Crop the output if needed\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[33;03m        h, w, c = all_layer_spacedims[li]\u001b[39;00m\n\u001b[32m     72\u001b[39m \u001b[33;03m        out_pad = out_pads[li]\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     78\u001b[39m \u001b[33;03m        else:\u001b[39;00m\n\u001b[32m     79\u001b[39m \u001b[33;03m            output_cropped = output  # For dense layers\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m         layer_output[li].append(\u001b[43moutput\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     83\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33m(\u001b[39m\u001b[38;5;132;01m%.1f\u001b[39;00m\u001b[33ms) \u001b[39m\u001b[33m'\u001b[39m % (time() - start), end=\u001b[33m'\u001b[39m\u001b[33m'\u001b[39m, flush=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     84\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33m Tot time = \u001b[39m\u001b[38;5;132;01m%.1f\u001b[39;00m\u001b[33m'\u001b[39m % (time() - start0), flush=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "####################### COMPUTE ################\n",
    "\n",
    "def reshape_flow_img(raveled_1chan_img):\n",
    "    img = raveled_1chan_img.reshape(input_shape)\n",
    "    img = np.stack([img], axis=0)  # Convert to 3 channels\n",
    "    return img\n",
    "\n",
    "# Prepare data processing\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std =[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "TOL = 0\n",
    "\n",
    "n_orig_imgs = tot_stims\n",
    "n_shifts = frames_per_stim\n",
    "n_shifted_imgs = n_orig_imgs * n_shifts\n",
    "\n",
    "# Batches for images and shifts\n",
    "maxBatchsize = 8  # Max number of images to be used as input simultaneously\n",
    "nBatches = int(np.ceil(n_shifted_imgs / float(maxBatchsize)))\n",
    "\n",
    "print('tot # of images:', n_orig_imgs, '*', n_shifts, '=', n_shifted_imgs)\n",
    "print('batchsize', maxBatchsize, '-- # of batches:', nBatches)\n",
    "\n",
    "layer_outputs = []\n",
    "instance_layer_outputs = []\n",
    "\n",
    "for li in range(1):\n",
    "    shape = [n_shifted_imgs] + [6636, 1, 1]\n",
    "    layer_outputs.append(np.zeros(shape, dtype='float32'))\n",
    "    shape_inst = np.append([N_INSTANCES], shape)\n",
    "    instance_layer_outputs.append(np.zeros(shape_inst, dtype='float32'))\n",
    "\n",
    "\n",
    "for insti in range(N_INSTANCES):\n",
    "    extX = flow_datasets[insti]\n",
    "    assert extX.shape[0] == n_shifted_imgs\n",
    "\n",
    "    print('INSTANCE', insti)\n",
    "    start0 = time()\n",
    "    layer_output = []\n",
    "    for li in range(1):\n",
    "        layer_output.append([])\n",
    "\n",
    "    for bb in range(nBatches):\n",
    "        start = time()\n",
    "        print(bb, end=' ', flush=True)\n",
    "\n",
    "        # Prepare batch\n",
    "        batch_images = []\n",
    "        for im in extX[bb * maxBatchsize: (bb + 1) * maxBatchsize]:\n",
    "            img = reshape_flow_img(im)\n",
    "            #img = Image.fromarray(np.uint8(np.transpose(img, (1, 2, 0))))\n",
    "            #img = preprocess(img)\n",
    "            batch_images.append(torch.Tensor(img))\n",
    "        batch = torch.stack(batch_images).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            BATCH_SIZE = batch.shape[0]\n",
    "            output = model(batch)\n",
    "            print(f\"Output shape: {output.shape}\")\n",
    "\n",
    "        # Collect outputs per layer\n",
    "        for li in range(1):\n",
    "            \"\"\"# Crop the output if needed\n",
    "            h, w, c = all_layer_spacedims[li]\n",
    "            out_pad = out_pads[li]\n",
    "            if output.ndim == 4:\n",
    "                # output shape: (batch_size, channels, height, width)\n",
    "                output_cropped = output[:, :, out_pad: out_pad + h, out_pad: out_pad + w]\n",
    "                # Rearrange to (batch_size, height, width, channels)\n",
    "                output_cropped = np.transpose(output_cropped, (0, 2, 3, 1))\n",
    "            else:\n",
    "                output_cropped = output  # For dense layers\"\"\"\n",
    "\n",
    "            layer_output[li].append(output.cpu())\n",
    "\n",
    "        print('(%.1fs) ' % (time() - start), end='', flush=True)\n",
    "    print(' Tot time = %.1f' % (time() - start0), flush=True)\n",
    "\n",
    "    # After processing all batches for this instance, concatenate outputs\n",
    "    for li in range(1):\n",
    "        # print(li)\n",
    "        print([l.shape for l in layer_output[li]])\n",
    "        layer_output[li] = np.expand_dims(np.expand_dims(np.concatenate(layer_output[li], axis=0), axis=2), axis=3)\n",
    "        print(layer_output[li].shape)\n",
    "        print(layer_outputs[li].shape)\n",
    "        layer_outputs[li] += layer_output[li]\n",
    "        instance_layer_outputs[li][insti] = layer_output[li]\n",
    "\n",
    "\n",
    "# Average over instances\n",
    "for li in range(1):\n",
    "    layer_outputs[li] /= N_INSTANCES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3256, 6636, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "print(layer_outputs[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activities per img: 0"
     ]
    }
   ],
   "source": [
    "################### SUMMARIZE ACTIVITY ###########\n",
    "\n",
    "print('Activities per img:', end=' ')\n",
    "all_neurons_maxs = []\n",
    "all_neurons_means = []\n",
    "all_per_img_output = []\n",
    "for li in range(1):\n",
    "    print(li, end='', flush=True)\n",
    "    layer_output_ = layer_outputs[li].copy()\n",
    "\n",
    "    layer_output_[layer_output_ < 0] = 0\n",
    "\n",
    "    nfmaps = layer_output_.shape[3]\n",
    "    # Reshape to [n_orig_imgs, n_shifts, nfmaps, -1]\n",
    "    orig_per_img_output = np.moveaxis(layer_output_, -1, 1).reshape([n_orig_imgs, n_shifts, nfmaps, -1])\n",
    "    orig_per_img_output = np.moveaxis(orig_per_img_output, 1, -1)\n",
    "\n",
    "    # Normalize each image by the max\n",
    "    layer_output_ /= np.maximum(layer_output_.max((1, 2, 3), keepdims=True), 1e-8)\n",
    "\n",
    "    per_img_output = np.moveaxis(layer_output_, -1, 1).reshape([n_orig_imgs, n_shifts, nfmaps, -1])\n",
    "    per_img_output = np.moveaxis(per_img_output, 1, -1)\n",
    "\n",
    "    tot_n_neurons = np.prod(layer_output_.shape[1:])\n",
    "\n",
    "    neurons_maxs = np.zeros(per_img_output.shape[1:3])\n",
    "    neurons_means = np.zeros(per_img_output.shape[1:3])\n",
    "\n",
    "    for imi in range(n_orig_imgs):\n",
    "        im_avgs = per_img_output[imi].mean(2)  # Averaging across time\n",
    "        neurons_maxs = np.maximum(neurons_maxs, im_avgs)\n",
    "        neurons_means += im_avgs\n",
    "    neurons_means /= n_orig_imgs\n",
    "\n",
    "    idxs = neurons_maxs.mean(1).argsort()\n",
    "\n",
    "    if li == 0:\n",
    "        all_neurons_maxs = neurons_maxs\n",
    "        all_neurons_means = neurons_means\n",
    "        all_per_img_output = orig_per_img_output\n",
    "    else:\n",
    "        all_neurons_maxs = np.concatenate([all_neurons_maxs, neurons_maxs], 0)\n",
    "        all_neurons_means = np.concatenate([all_neurons_means, neurons_means], 0)\n",
    "        all_per_img_output = np.concatenate([all_per_img_output, orig_per_img_output], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "2000\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "############# SAMPLE NEURONS ###########\n",
    "\n",
    "nfmaps, n_neurons_per_fmap = all_neurons_maxs.shape\n",
    "layer_is_per_fmap = np.concatenate([li * np.ones(nf) for li, nf in enumerate([1])])\n",
    "np.random.seed(seed)\n",
    "\n",
    "maxsmean = all_neurons_maxs.mean(1)\n",
    "nonzero_indices = (~np.isclose(maxsmean, 0)).sum()\n",
    "n_fmaps_to_sample_ = min(n_fmaps_to_sample, nonzero_indices)\n",
    "print(n_fmaps_to_sample)\n",
    "if fmap_samp_method == 'maxFr':\n",
    "    probabilities = maxsmean / maxsmean.sum()\n",
    "    top_fmaps = np.random.choice(range(nfmaps), n_fmaps_to_sample_, replace=False, p=probabilities)\n",
    "else:\n",
    "    raise ValueError('Invalid fmap_samp_method')\n",
    "\n",
    "# Pick active neurons in each of these feature maps\n",
    "sampled_neurons = []\n",
    "\n",
    "samples_per_fmap = min(samples_per_fmap, all_neurons_means.shape[1])\n",
    "print(samples_per_fmap)\n",
    "for fi in top_fmaps:\n",
    "    if neur_samp_method == 'maxNr':\n",
    "        neuron_vals = all_neurons_maxs[fi]\n",
    "        nonzero_neurons = (~np.isclose(neuron_vals, 0)).sum()\n",
    "        samples_per_fmap_ = min(samples_per_fmap, nonzero_neurons)\n",
    "        probabilities = neuron_vals / neuron_vals.sum()\n",
    "        top_nis = np.random.choice(range(n_neurons_per_fmap), samples_per_fmap_, replace=False, p=probabilities)\n",
    "    else:\n",
    "        raise ValueError('Invalid neur_samp_method')\n",
    "    sampled_neurons += list(fi * n_neurons_per_fmap + top_nis)\n",
    "sampled_neurons = np.array(sampled_neurons)\n",
    "n_neurons_to_pick = len(sampled_neurons)\n",
    "print(n_neurons_to_pick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mini_act_i3_n2000_SCL0_4_TL37_readout_maxFr_maxNr_seed1\n",
      "tensor4d_mini_act_i3_n2000_SCL0_4_TL37_readout_maxFr_maxNr_seed1.npy Saved.\n",
      "neurons_used_mini_act_i3_n2000_SCL0_4_TL37_readout_maxFr_maxNr_seed1.npy Saved.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "######### BUILD TENSOR ##########\n",
    "\n",
    "def get_neuron_pos(ni):\n",
    "    \"\"\"From sampled indices ni, get original indices back (layer index, fmap, posi, posj, raveled_idx)\"\"\"\n",
    "    fi = ni // n_neurons_per_fmap\n",
    "    li = int(layer_is_per_fmap[fi])\n",
    "    ij = ni % n_neurons_per_fmap\n",
    "    h, w, _ = (6636, 1, 1)\n",
    "    ii = ij // w\n",
    "    jj = ij % w\n",
    "    return li, fi, ii, jj, ij\n",
    "\n",
    "assert n_orig_imgs // NDIRS == len(categories)\n",
    "\n",
    "tensorX = np.zeros((n_neurons_to_pick, len(categories), NDIRS, n_shifts))\n",
    "neurons_used = np.empty((n_neurons_to_pick, 5), dtype='int')\n",
    "\n",
    "# Collect PSTs for those sampled neurons\n",
    "for nii, ni in enumerate(sampled_neurons):\n",
    "    li, fi, ii, jj, posi = get_neuron_pos(ni)\n",
    "    neurons_used[nii] = [li, fi, ii, jj, posi]\n",
    "\n",
    "    for cati in range(len(categories)):\n",
    "        pst = all_per_img_output[cati * NDIRS: (cati + 1) * NDIRS, fi, posi, :]\n",
    "        tensorX[nii, cati] = pst\n",
    "\n",
    "SUFFIX = f\"{model_name}_{LAYER_TYPE}_i{N_INSTANCES}_n{n_neurons_to_pick}_SCL{str(scl_factor).replace('.', '_')}_TL{trial_len}_{'_'.join(block)}_{fmap_samp_method}_{neur_samp_method}\"\n",
    "if seed > 0:\n",
    "    SUFFIX += f'_seed{seed}'\n",
    "\n",
    "print(SUFFIX)\n",
    "\n",
    "directory = '../data/sampled_data'\n",
    "os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "if os.path.exists(f'../data/sampled_data/tensor4d_{SUFFIX}.npy'):\n",
    "    print(\"Files already exist, please delete them to prevent conflicts.\")\n",
    "else:\n",
    "    \n",
    "    np.save(f'../data/sampled_data/tensor4d_{SUFFIX}.npy', tensorX)\n",
    "    print(f'tensor4d_{SUFFIX}.npy Saved.')\n",
    "\n",
    "    np.save(f'../data/sampled_data/neurons_used_{SUFFIX}.npy', neurons_used)\n",
    "    print(f'neurons_used_{SUFFIX}.npy Saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
